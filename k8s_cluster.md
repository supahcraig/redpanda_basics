# Creating a Redpanda cluster in EKS

This is taken from the public docs, but slightly modified to make the cluster name dynamic based on the os user env variable `USER`.   Should take 35 minutes to complete.

May want to change `--name redpanda` to something more personal like `--name ${USER}-redpanda` but this may cause problems during the helm install?

This environment variable will help ensure the resources that get created won't collide with other things and cause you weeping and gnashing of the teeth.  It can be whatever you like, but choose it to be relatively unique for your AWS acct.

```
export REDPANDA_CLUSTER_NAME=${USER}-redpanda
```

This will create 3 stacks in CloudFormation 
First stack: `eksctl-cnelson-<REDPANDA_CLUSTER_NAME>`
* EKS cluster
* VPC & related subnets, routes, etc
* Cluster Service IAM role called `eksctl-<REDPANDA_CLUSTER_NAME>-cluster-ServiceRole-6HNR2GX507YN` where that trailing ID is dynamically generated by `eksctl`.
* 2 policies are created which are attached to that role:
  * `eksctl-<REDPANDA_CLUSTER_NAME>-cluster-PolicyCloudWatchMetrics`
  * `eksctl-<REDPANDA_CLUSTER_NAME>-cluster-PolicyELBPermissions`

Second stack: `eksctl-<REDPANDA_CLUSTER_NAME>-addon-iamserviceaccount-kube-system-aws-node`
* IAM role named `eksctl-<REDPANDA_CLUSTER_NAME>-addon-iamserviceacco-Role1-1X6EMYQXXXCBZ`

Third stack: `eksctl-<REDPANDA_CLUSTER_NAME>-nodegroup-standard-workers`
* Managed node group (and thus the EC2 instances)
* Launch template
* IAM role called `eksctl-c<REDPANDA_CLUSTER_NAME>-nodegroup-NodeInstanceRole-OQU9D45V0PGL`
  * policy `eksctl-<REDPANDA_CLUSTER_NAME>-nodegroup-standard-workers-PolicyExternalDNSChangeSet`
  * policy `eksctl-<REDPANDA_CLUSTER_NAME>-nodegroup-standard-workers-PolicyExternalDNSHostedZones`

```
eksctl create cluster --with-oidc --name ${REDPANDA_CLUSTER_NAME} \
    --external-dns-access \
    --nodegroup-name standard-workers \
    --node-type m5.xlarge \
    --nodes 3 \
    --nodes-min 3 \
    --nodes-max 4 \
    --tags "owner=${USER}"
```

Verify you can connect to the cluster:

```
aws eks update-kubeconfig --region us-east-2 --name ${REDPANDA_CLUSTER_NAME}
kubectl get service
```


## Create Service Account

This creates a CloudFormation stack called `eksctl-<REDPANDA_CLUSTER_NAME>-addon-iamserviceaccount-kube-system-ebs-csi-controller-sa`
* IAM role `AmazonEKS_EBS_CSI_DriverRole_<REDPANDA_CLUSTER_NAME>`
  * uses an AWS managed policy

```
eksctl create iamserviceaccount \
    --name ebs-csi-controller-sa \
    --namespace kube-system \
    --cluster ${REDPANDA_CLUSTER_NAME} \
    --attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy \
    --approve \
    --role-only \
    --role-name AmazonEKS_EBS_CSI_DriverRole_${REDPANDA_CLUSTER_NAME} \
    --tags "owner=${USER}"
```

This stack create can cause problems.   It may fail telling you that the role already exists.   That will almost certainly cause you a problem later on, see the troubleshooting guide in this repo.

But it is throwing an error because this service acct already exists...lets see how far we can get beore it becomes a problem.   Solution would be to either create a new one OR somehow attach the already existing service acct to my new cluster.  Based on the output from the initial eksctl cluster create, this stack is brought up as part of initial stack, that's why it was already there.
ERROR MESSAGE:

```
2023-03-13 13:39:58 [ℹ]  waiting for CloudFormation stack "eksctl-cnelson-redpanda-addon-iamserviceaccount-kube-system-ebs-csi-controller-sa"
2023-03-13 13:39:58 [ℹ]  1 error(s) occurred and IAM Role stacks haven't been created properly, you may wish to check CloudFormation console
2023-03-13 13:39:58 [✖]  waiter state transitioned to Failure
Error: failed to create iamserviceaccount(s)
```




Next create this addon, which may also already be present through previous steps.

```
AWS_ACCOUNT_ID=`aws sts get-caller-identity | jq -r '.Account'`

eksctl create addon \
    --name aws-ebs-csi-driver \
    --cluster ${REDPANDA_CLUSTER_NAME} \
    --service-account-role-arn arn:aws:iam::${AWS_ACCOUNT_ID}:role/AmazonEKS_EBS_CSI_DriverRole_${REDPANDA_CLUSTER_NAME} \
    --force
```

Next update teh security group.  Our docs are not at all clear about what the inbound IP range or secuity group should be.  `group-id` was found by looking at the EC2 instances in the AWS console.   Could probably sus it out with a desribe & some jq.   Left as a TODO

Find the security group name, then export it to an environment variable.

```
export REDPANDA_SG=$(aws ec2 describe-instances --filter "Name=tag:aws:eks:cluster-name,Values=cnelson-redpanda" | jq -r '.Reservations[].Instances[].NetworkInterfaces[].Groups[].GroupId' | uniq -c | tr -s ' ' | cut -d ' ' -f 3)
```


```
aws ec2 authorize-security-group-ingress \
    --group-id ${REDPANDA_SG} \
    --ip-permissions "[ \
                        { \
                          \"IpProtocol\": \"tcp\", \
                          \"FromPort\": 30081, \
                          \"ToPort\": 30082, \
                          \"IpRanges\": [{\"CidrIp\": \"0.0.0.0/0\"}] \
                        }, \
                        { \
                          \"IpProtocol\": \"tcp\", \
                          \"FromPort\": 31644, \
                          \"ToPort\": 31644, \
                          \"IpRanges\": [{\"CidrIp\": \"0.0.0.0/0\"}] \
                        }, \
                        { \
                          \"IpProtocol\": \"tcp\", \
                          \"FromPort\": 31092, \
                          \"ToPort\": 31092, \
                          \"IpRanges\": [{\"CidrIp\": \"0.0.0.0/0\"}] \
                        } \
                      ]"

```


## Helm Install

```
helm repo add redpanda https://charts.redpanda.com && helm repo add jetstack https://charts.jetstack.io && helm repo update && helm install cert-manager jetstack/cert-manager  --set installCRDs=true --namespace cert-manager  --create-namespace
```


```
export DOMAIN=customredpandadomain.local && \
helm install redpanda redpanda/redpanda -n redpanda --create-namespace \
  --set auth.sasl.enabled=true \
  --set "auth.sasl.users[0].name=superuser" \
  --set "auth.sasl.users[0].password=secretpassword" \
  --set external.domain=${DOMAIN} --wait
```

or w/o persistent volumes:

```
export DOMAIN=customredpandadomain.local && \
helm install redpanda redpanda/redpanda -n redpanda --create-namespace \
  --set storage.persistentVolume.enabled=fales \
  --set auth.sasl.enabled=true \
  --set "auth.sasl.users[0].name=superuser" \
  --set "auth.sasl.users[0].password=secretpassword" \
  --set external.domain=${DOMAIN} --wait
```



## Helm cleanup

```
helm uninstall redpanda -n redpanda
kubectl delete ns redpanda
```


----

## notes

the first `eksctl` command creates a stack in CloudFormation called `eksctl-cnelson-redpanda-cluster`
It creates a VPC & related subnets, routes, etc, and some IAM roles/policies, and the EKS cluster

At the same time additional stacks are created:
`eksctl-cnelson-redpanda-nodegroup-standard-workers`
This includes an EKS node group (and also the EC2 instances?), launch templates, and more IAM stuff

There is one other stack created called `eksctl-cnelson-redpanda-addon-iamserviceaccount-kube-system-aws-node` 
this creates one IAM role.

The second `eksctl` command creates this stack: `eksctl-cnelson-redpanda-addon-iamserviceaccount-kube-system-ebs-csi-controller-sa`
All it does is create an IAM role called `AmazonEKS_EBS_CSI_DriverRole`
If this role already exists, it will cause issues.   The role name probalby needs to be more dynamic.


